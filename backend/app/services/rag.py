import logging
from openai import AsyncOpenAI
import os


class RAG:
	
	"""
	
	Service that takes a user query and provides a response based on relevant document chunks.
	
	"""
	
	def __init__(self, db, embedder, model: str = "gpt-4o-mini"):
		
		"""
	
		Initialize the service with the required dependencies and model.

		:param db: The Database instance to fetch relevant chunks.
		:type db: Database
		:param embedder: The EmbeddingService instance to generate query embeddings.
		:type embedder: EmbeddingService
		:param model: The OpenAI model to use for generating the answer.
		:type model: str
		
		"""
		
		self.db = db
		self.embedder = embedder
		self.model = model
		self.openai_client = AsyncOpenAI(api_key = os.getenv("OPENAI_API_KEY"))
		
		# Check if the OpenAI API key is set
		if not self.openai_client.api_key:
			raise ValueError("OPENAI_API_KEY not found in environment variables.")
	
	
	@staticmethod
	async def build_prompt(chunks: list[dict], query: str) -> str:
		"""
		
		Build a prompt string from relevant chunks and the user query.

		:param chunks: List of dicts containing document content and metadata
		:type chunks: list[dict]
		:param query: User's question
		:type query: str
		
		:return: A string prompt
		:rtype: str
		
		"""
		
		context = "\n\n".join(
				f"[{chunk['document_type']} - Page {chunk['page_number']}]\n{chunk['content']}"
				for chunk in chunks
				)
		
		prompt = (
			f"You are an assistant helping with HOA-related questions. Make sure to include the source and page number "
			f"as \"Source: [source] Page [page] \" in your response\n\n"
			f"Context:\n{context}\n\n"
			f"Question: {query}\n\n"
			f"Answer:"
			)
		
		# Return the formatted prompt
		return prompt
	
	
	async def generate_answer(self, prompt: str) -> str:
		
		"""
		
		Use the OpenAI API to generate an answer based on the prompt.

		:param prompt: The prompt string that provides context and the user's question.
		:type prompt: str
		
		:return: The answer generated by OpenAI.
		:rtype: str
		
		"""
		
		try:
			
			# Generate the answer using OpenAI's chat completion API
			response = await self.openai_client.chat.completions.create(
					model = self.model,
					messages = [
						{"role": "system", "content": "You are a helpful assistant."},
						{"role": "user", "content": prompt},
						],
					temperature = 0.3
					)
			
			# Return the generated answer
			return response.choices[0].message.content.strip()
		
		# Handle any exceptions that occur during the API call
		except Exception as e:
			
			# Log the error
			logging.error(f"LLM generation failed: {str(e)}")
			
			# Raise a runtime error with the exception message
			raise RuntimeError(f"LLM generation failed: {str(e)}")
	
	
	async def answer_query(self, query: str, hoa_code: str) -> str:
		
		"""
		
		Takes a user query, retrieves relevant chunks, formats a prompt,
		generates an answer, and returns the final response with sources.

		:param query: The user's question
		:type query: str
		:param hoa_code: The HOA code to filter documents by
		:type hoa_code: str
		
		:return: The answer to the query, including sources
		:rtype: str
		
		"""
		
		try:
			
			# Step 1: Generate embedding for the query
			query_embedding = await self.embedder.get_query_embedding(query)
			
			# Step 2: Fetch relevant chunks (with context) from the database
			relevant_chunks = await self.db.get_relevant_chunks_with_context(query_embedding, hoa_code)
			
			# print(relevant_chunks)
			#
			# print(len(relevant_chunks))

			# Step 3: Build the prompt for the LLM
			prompt = await self.build_prompt(relevant_chunks, query)
			
			# Step 4: Generate an answer from OpenAI
			answer = await self.generate_answer(prompt)

			# Step 5: Return full response (answer + sources)
			return f"{answer}"
		
		# Handle any exceptions that occur during the process
		except Exception as e:
			
			# Log the error
			logging.error(f"Failed to answer query: {str(e)}")
			
			# Return a generic error message
			return "Sorry, something went wrong while processing your query."
